\documentclass[12pt]{article}

\title{Research training portfolio}
\author{Wessel Stoop}

\usepackage{covington}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{float}

\renewcommand{\familydefault}{\sfdefault}

\let\stdsection\section
\renewcommand\section{\newpage\stdsection}

\begin{document}
\maketitle

\begin{table}[h]
\begin{tabular}{ll}
Name&Wessel Stoop\\
Student numbers&s0808709 (Nijmegen), u1249664 (Tilburg)\\
Project&Fowlt, a context-sensitive spelling checker\\
Supervisor&Antal van den Bosch\\
Period&September - December 2012\\
\end{tabular}
\end{table}

\section{Introduction}
Fowlt is the English version of the Dutch spelling corrector Valkuil.net. Both Valkuil and Fowlt are unlike most well-known spelling correctors (like the one in Microsoft Word): whereas these spelling correctors mostly try to find errors by comparing all words to a built-in dictionary and flag the word as an error if they can't find a match, Valkuil and Fowlt are 'context-sensitive'. This entails that they also take into account the context, which is the words around every word. If, for a particular word, they expected another word based on the context, and the algorithm is really certain about it, it is flagged as an error. This means, for example, that Fowlt is able to replace the incorrect "there' in "there really nice people" into "they're" simply because "there" usually isn't followed by "really nice people", while "they're" is. 
\\\indent
To be able to make these kinds of correction suggestions, Fowlt makes use of language models. These models are created by giving lots of texts to the machine learning software TiMBL and WOPR. It's on the basis of these texts the model knows "there" mostly is not followed by "really nice people". However, this also means that if the context of a particular error is very different from what the model has seen in the training data, it won't be able to correct the error. This means users should be aware of the fact that, although Fowlt can recognize more kinds of errors than regular spelling correctors, it might still miss a few. 
\\\indent
Although most of software was already there when I started, this project has been a long journey, with many problems. In this portfolio, I hope to give the reader an idea of what I did, what I learned, and what went wrong. On top of that, I did a little study on Fowlt's confusible checkers. I report this study in section 4.
\\\indent

\section{What I have done}
The best way to get a good overview of what I have done is probably to look at Fowlt's internal structure, so that's what I present in the next section. The first part of my internship basically consisted of adding the modules described below, one by one. To get a better idea of what parts took more time to 'get right', I also added a section about problems I encountered.

\subsection{Project structure}
Fowlt exists of multiple little spelling correctors, specialized in doing one thing. These spelling correctors are called 'modules'. Fowlt consists of four context-free modules (relatively similar to normal spelling correctors functionality) and, as I write this, sixteen context-sensitive modules. This number will probably be different when you read this, though, as new context-sensitive modules are created easily, and can be removed if it turns out they're not accurate enough. Although it is possible to run Fowlt as a stand-alone application, most people will use it as a web application. I will devote a subsection to this 'web shell' around Fowlt as well.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{fowlt_chain.png}
\caption{Fowlt's architecture}
\label{fig:chain}
\end{figure}

\subsubsection{The processchain}
In Fowlt's main script, fowlt\_processchain.py, all modules are started up, their outputs are collected and are all saved in xml form. Making changes in this script (based on the one for Valkuil, written by Maarten van Gompel) to make sure the new English based modules worked correctly, is one of the main things I did during my internship. 

\subsubsection{Errorlist module (context-free)}
Martin Reynaert provided us with a large list of common typos and their corrections. This module, written by me, checks for every word in the input whether it's in the errorlist, and adds the correction when it is.

\subsubsection{Lexicon module (context-free)}
Google provided us with an enormous list of word frequencies on the internet in 2008. The LexiconChecker checks for every word in the input whether there's a word on the frequencylist that is very similar to it, but much more frequent.

\subsubsection{Runon module (context-free)}
The Runon module also uses Google's list, but uses it to check if any spaces were forgotten in the input. This is done by looking whether splitting up long words produces two words which both are much more frequent that the original word.

\subsubsection{Split module (context-free)}
The Split module is the exact opposite of the RunonChecker: instead of looking whether any spaces are forgotten, it checks whether any spaces have to be added. This is done by looking whether each combination of two words produces a word which is much more frequent than the two original words.

\subsubsection{WOPR module(context-sensitive)}
The WOPR module contacts WOPR, running on a server on localhost or elsewhere. It gives it the context of each word, but not the words themselves. If WOPR predicts another word with much certainty, this word is added as a correction.  

\subsubsection{The Confusible modules (context-sensitive)}
The confusible modules work similar to the WOPR module, with the only difference that they are trained on one specific language problem, instead of on language in general. As I write this, there are fifteen active confusible modules, all focussing on their own expertise: lose-loose, it's-its, you're-your, than-then, who-which-that, whether-weather, effect-affect, lie-lay, they're-their-there, don't-doesn't,to-too-two, advice-advise, any-some, less-fewer, practice-practise and chose-choose.
\\\indent
A confusible module trained on the difference between 'then' and 'than' for example filters out these two words and their contexts, sends these contexts to a Timblserver running somewhere. This Timblserver then says which of the two it would predict on the basis of the context. If this prediction is different from the actual word and the server is very sure about this, the word is flagged as an error.

\subsubsection{Fowlt as a web application}
Fowlt can also (and, probably, will mostly) be used as a website. In this web variant, Fowlt works exactly as described above, but some extra mechanisms are built around it to make sure it works and looks nice in the browser. The transition from a standalone application to a website is a two-step process. Firstly, Fowlt is made available online webservice using the software CLAM. Among other things, CLAM adds the possibility to use Fowlt as a RESTful service. However, ordinary users don't want to send HTTP requests or read xml files when they want their texts corrected. This is solved in the second step, which is the Fowlt.net interface. In this interface, users can paste their texts, and see the mistakes flagged in red in these texts themselves. The interface even adds functionality to correct any errors. Behind the screens, the interface communicates with the CLAM webservice.
\\\indent
Most of the functionality described above was already working for Valkuil, and so only had to be adapted to English to be function within Fowlt. One feature that I created for Fowlt that Valkuil doesn't have (yet), is an error tolerance slider. With this slider, users can change on the fly how confident Fowlt has to be about an error before it displays it. The implementation of this slider was, however, not without problems. See section \ref{formulas} more information on that.

\subsection{Problems encountered}

\subsubsection{The original confusible module could only handle two confusibles}
Most confusibles consist of two words only, but English has two that consist of three: they're-there-their and to-too-two. The original c-code could not handle confusibles with more than two words, so while the original plan was to either use Valkuil's c-code or rewrite it in Python, the quickest solution here was me diving into the c-code of my supervisor and add support for three.

\subsubsection{Problems with Ucto}
Although Ucto includes a configuration file for English, it didn't produce the correct output right out of the box. For example, for the It's-ItsModule to work, it is essential that "it's" is recognized as one word, which is not the case in original English configuration file. After a lot of tweaking of the regular expressions, I now have a tokenizer that produces output which can be used by all modules.

%Deze paragraaf is twijfelachtig
%\subsubsection{The ConfusibleModules weren't good enough}
%I didn't only train the ConfusibleModules, I also tested them with a program written by my supervisor. This program takes 10\% from the training data and saves it in a seperate file, trains TiMBL on the remaining 90\%, and then returns the percentage of correct predictions the resulting model can make about the 10\%. The results differed for the various confusibles, but the models were correct roughly 85-90\% of the time. This may seem a lot, but for a spellingchecker to be useful it's essential that a model can have an accuracy of over 99\%. Following the advice of my supervisor, I did two things to accomplish this: (1) I retrained TiMBL on the balanced training sets, which entails that there is an even amount of examples for each word the confusible consists of, and (2) I made the ConfusibleChecker only add a correction when it is very sure. Especially the latter solution increases the accuracy dramatically, but at the price of finding a lot less errors.

\subsubsection{WOPR didn't behave as expected}
WOPR wasn't expected to give a lot of problems: it works well in Valkuil, so simply retraining it on English data should have been enough. Unfortunately, this turned out not be the case: although WOPR was trained with exactly the same settings, and was used in exactly the same way, it always returned that it didn't discover any errors. As I write this, this problem still isn't fully solved, but we did discover three things: 

\begin{itemize}

\item The problem is not in the communication between Fowlt and WOPR, as WOPR starts to respond when you tell it it can also give suggestions that have a larger Levenshtein Distance than 1.
\item The problem also isn't in the training data, as WOPR does find errors when it is used from the command line (instead of while communicating with Fowlt in server mode).
\item WOPR finds more errors when you turn off the setting \emph{min\_ratio:100}. This setting makes sure its suggestion is 100 times more frequent than the original word. It is completely unclear why turning this setting on and off makes such a big difference, as it is always on for Valkuil.

\end{itemize}

\subsection{All browsers and operating systems treat css differently}
Fowlt's web interface, designed by Monica Hajek, is very similar to Valkuil's interface. I thought that simply changing the logo and some colors would be all interface-related things I had to do. I couldn't have been more wrong: Monica wanted every GUI element to be on exactly the right place, pixel-perfect. This meant that every single element I had added (and even some elements I hadn't added) had to be repositioned.\\\indent
Having used css several times before, I thought this entailed changing some values and adding some things to Fowlt's stylesheet. Unfortunately, this turned out to be a very hard, frustrating and time-consuming task, as no two browsers behave exactly the same. This means that having all positioning correct in one browsers means having all positioning horribly wrong in another. And to make things even more complicated, things differ per operating system as well: although you can use Chrome on Windows, Mac and Linux, Fowlt looks very different on each of them.\\\indent
After many frustrated mornings, I decided to give up on this problem, and instead focus on the most used browser-operating system combination, which is Windows+Chrome. Monica provided me with a normalizer style sheet, which claims to eliminate browsers differences and indeed fixes some problems, but at the moment the problem is far from solved.

\subsubsection{Most modules didn't provide confidences measures} \label{formulas}
Implementing the confidence slider described above was a hard and labor-intensive process. Not only did I have to create a working slider in Javascript that behaved like expected in all browsers, this slider also had to look like Monica had envisioned it, and the position of the handle had to be linked to both the errors marked in the text and the errors in the menu (directly above the slider). Before this could work, however, the slider needed to be able to extract the confidence for each error from Fowlt's output xml. This information about confidence originally wasn't there either, so I had to tell the process chain to ask each module for this information, and add it to the right place in the output. All modules subsequently had to be changed so they actually did return their confidence for each error. In other words, imlementing the slider was a long journey that made me revisit all layers of Fowlt's architecture.\\\indent
The biggest problem in this process, however, is that most modules not only didn't return some confidence measure for each error, they actually didn't calculate one. This not only meant I had to go back to the C-code of the original modules, but also that I had to come up with formulas to represent exactly how confident the modules are. I would like to use this final subsection to explain in detail the formulas I have chosen:

\paragraph{Lexicon Checker}

\[
1 - \frac{wf}{cf/ft}
\]

The lexicon checker gives a correction \emph{c} for a word \emph{w} if \emph{c} is very similar to \emph{w}, but \emph{frequency threshold} times more frequent. As I write this, the frequency threshold (ft) used for the lexicon checker is 10.000. This confidence formula reflects how many times the correction frequency (cf) is 10.000 times more frequent than the frequency of the original word (wf). Thus, if the correction is only 10.000 times more frequent, the result will be 0, if it is 20.000 more frequent, it will be 0.5, etc. The larger the difference between the original word and the correction, the closer to 1 the result will be.

\begin{table}[h]
\begin{tabular}{|l|lll|}
\hline
&word frequency (wf)&correction frequency (cf)&frequency threshold (ft)\\ 
\hline
anticcipate&1*&2.710.230&10.000\\
newwspaper&876&15.200.566&10.000\\
\hline
\end{tabular}
\end{table}

anticcipate $\rightarrow$ anticipate

\[
1 - \frac{1}{2.710.230/10.000} = 0.996
\]

newwspaper $\rightarrow$ newspaper

\[
1 - \frac{876}{15.200.566/10.000} = 0.43
\]

* If the word frequency is lower than 1, it is assumed to be 1.

\paragraph{Runon Checker}

\[
1 - \frac{1}{(\frac{(lf+rf)/2}{t})}
\]

The left fraction (lf) and the right fraction (rf) of a word represent how often the left and the right part of that word are part of a bigger word. If for these two parts the number is above a threshold (3000 as I write this), the Runon Checker suggests the two parts are probably words on their own. This confidence formula reflects how high these fractions are. It first calculates the mean fraction and relates this to the threshold (t). The result is then transformed into a number between 1 and 0.

\begin{table}[h]
\begin{tabular}{|l|lll|}
\hline
&left fraction (lf)&right fraction (rf)&threshold (t)\\ 
\hline
fightscrime&37.274,75&94.063,234&3000\\
unexpectedhorror&81.098,078&55.204,098&3000\\
\hline
\end{tabular}
\end{table}

fightscrime $\rightarrow$ fights crime

\[
1 - \frac{1}{(\frac{(37.274,75+94.063,234)/2}{3000})} = 0.954
\]

unexpectedhorror $\rightarrow$ unexpected horror

\[
1 - \frac{1}{(\frac{(81.098,078+55.204,098)/2}{3000})} = 0.956
\]

\paragraph{Split Checker}

\[
1 - \frac{1}{(\frac{cf}{((fpw+fcw)/fr)/2})}
\]

The Split Checker combines every word with the word before it, and checks if the frequency of the result is higher than the frequencies of both original words. Before this comparison is done, the frequencies of the original words are divided by the frequency ratio (2 as I write this), so the module will respond more often. This confidence formula reflects the difference between the mean frequency of both original words (fwp and fcw) and the frequency of the new word (cf). It first calculates this mean, and also takes the frequency ratio (fr) into account. This mean is then compared to the frequency of the mean of the combination, and the result is then transformed into a number between 1 and 0.

\begin{table}[h]\footnotesize
\begin{tabular}{|l|lll|}
\hline
&combination frequency (cf)&previous word frequency (pwf)&current word frequency (cwf)\\ 
\hline
unexp ected&1054275&2124&7223\\
flabberg asted&97389&0&1913\\
\hline
\end{tabular}
\end{table}

unexp ected $\rightarrow$ unexpected

\[
1 - \frac{1}{(\frac{1054275}{((2124+7223)/2)/2})} = 0.982
\]

flabberg asted $\rightarrow$ flabbergasted

\[
1 - \frac{1}{(\frac{97389}{((0+1913)/2)/2})} = 0.995
\]


\section{What I have learned}
Although had some experience with both programming and language technology, and even with some software co-developed by my supervisor, this internship turned out to be an adventure full of novelty: no single semester in my Radboud University career had such a high skills-acquired/day-ratio as this one. In the following sections, I give a non-exhaustive overview of what I learned while working on Fowlt.

\subsection{Working with Linux and SSH}
The novelty already started with something as non-trivial as the choice of operating system. As a Microsoft Windows user, I was surprised to find out all machine learning software I needed was developed and best used on Linux (see the next section). I only had a few hours of experience with Ubuntu, so I had to invest several hours into installing and getting more familiar with it. Ubuntu's GUI turned out to be almost identical to Windows', so most time went into learning the terminal commands. Unlike the commands for Windows' command prompt, Linux commands are essential for having full control. The first hours of the internship were therefore spent figuring out basic actions like \emph{ls} (showing folders and files in a directory) and \emph{mv} (renaming and moving files). This knowledge was also essential later, when I needed to control external servers over an SSH connection, as these Linux machines can be controlled with commands only. Here again, it took some time to figure out how to make the servers do what I wanted - transferring a python script from my machine to a server, for instance. Fortunately, working with Linux became routine quickly.

\subsection{Working with machine learning software}
Fowlt uses the machine learning software TiMBL, TiMBLserver and WOPR. TiMBL consists of several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification with feature weighting suitable for symbolic feature spaces, and IGTree, a decision-tree approximation of IB1-IG. TiMBLserver adds server functionality to TiMBL. WOPR is a wrapper around the k-nearest neighbor classifier in TiMBL, offering word prediction and language modeling functionalities. Trained on a text corpus, WOPR can predict missing words, report perplexities at the word level and the text level, and generate spelling correction hypotheses. For Fowlt, only WOPR's spelling modus is used, of course.
\\\indent
Unfortunately, all three programs have very little documentation, and documentation that does exist often assumes background knowledge. With the help of my supervisor and Maarten van Gompel I learned how to (1) make language models by training the algorithms on large amounts of texts, (2) use these models to predict new words (and in my case, find spelling errors) and (3) test the model's accuracy. During my internship, I've repeated these three steps numerous times.
\\\indent
Besides TiMBL, TiMBLserver and WOPR, I also learned to use Ucto, software to tokenize texts so TiMBL and WOPR can handle them, and the PyNLPl Python library, which can read and create FoLiA-XML (among other things). FoLiA-XML is the XML-format used by Fowlt's output.

\subsection{Working with GitHub}
Working with multiple people on the same software can cause quite a lot of practical problems. Version control system Git solves these problems by keeping track of which version everyone of working on, providing tools to easily merge the work when done, and to remember older versions of the project, among other things. Although I had used Git for the latter function in personal hobby projects, I'd never worked together with other people on programming tasks. During this internship I learned (1) this can be done easily by using Git in combination with GitHub and (2) how to handle GitHub. GitHub is a website that offers open source projects a free, central and always reachable place for a Git repository, and also provides tools to look into this repo and its history online.

\subsection{Working with Django and CLAM, setting up an NLP webservice}
Although I do have experience with web development, everything I had to do to get Fowlt online was completely new to me. Whereas I was used to web programming language PHP, Fowlt works with Django, a Python library for web development. Fortunately, the Django documentation turned out to be very good and clear, so I got the hang of it quickly. In fact, Django turned out to be so elegant and easy that I might use it in future projects as well. The fact that most code needed could be copied from Valkuil made this part extra easy. CLAM, which turns an NLP application into a webservice, also turned out to be easy to use, and has a good documentation. I had Fowlt working with CLAM within an hour.
\\\indent
However, the real problems with setting up an NLP webservice were not related to Django or CLAM, but to all kinds of unexpected behavior of the university's servers. For example, the user groups on Applejack were not set up correctly when I started, prohibiting me to reach some locations, both Spitfire and Applejack reset file permissions when files are updated, and the server software caches server settings, even after updating the configuration file. In retrospect, the problems were minor and could be solved easily (either by me or by an administrator), but it took me days to figure out what the problems were and how to solve them.

\subsection{General}

Besides these general themes, there were several similar learning moments per day. Things that I learned include how to use the Google Calendar, the concept IRC (of which I had only a very basic understanding), the fact that bash shell aliases make your life a lot easier, how to make Latex show pictures exactly where you want them, what Python list comprehensions and lambda functions really are, and how to compile C code (and that it should be compiled in the first place).

\section{Research paper}

\subsection*{Abstract}

For many confusibles, like "\emph{you're} versus \emph{your}", one option is much more frequent than the other. This poses a problem when one tries to create a context-sensitive memory-based confusible corrector, as having more data for one option will make it more likely that the confusible corrector will predict that option. In the literature it has been suggested that balancing the training data, so that it has exactly the same amount of examples for each option, might improve results of classifiers in general. In this paper I show this does not hold for classifiers in a spelling correction environment.

\subsection{Introduction}

This paper results from my attempts to solve a rather practical issue while building Fowlt, an online spelling corrector freely available on the internet. Fowlt, just like its Dutch equivalent Valkuil, is a context-sensitive spelling corrector. This context-sensitivity makes it possible to detect errors like the following:

\begin{examples}

\item There really nice people. \emph{Intended: they're.}
\item I'd like too see you two. \emph{Intended: to and too.}
\item I don't want to loose you. \emph{Intended: lose.}

\end{examples}

These errors, in which one word is confused with another (often homophone) word, are called \emph{confusible errors}. For most common confusible errors, Fowlt contains specialized modules, which I will call \emph{confusible correctors} in the remainder of this paper. These confusible correctors cannot detect the errors they're supposed to detect by simply looking at the text word for word, because the error is in the relation of a particular word with the other words in the sentence. Therefore, they detect them by looking at the context of the word, and see if that differs from the contexts they have seen in the past for that word. If it, on the basis of that context, would predict another word in that position, and is really certain about it, it flags the word as an error. This means, for example, that Fowlt is able to replace the incorrect "there" in the last example into "they're", simply because "there" usually isn't followed by "really nice people", while "they're" is. Fowlt has many confusible correctors like this, but for this paper I will limit myself to six examples. These are the confusibles they are specialized in: 

\begin{itemize}
\item \emph{advice} versus \emph{advise}
\item \emph{lose} versus \emph{loose}
\item \emph{practice} versus \emph{practise}
\item \emph{than} versus \emph{then}
\item \emph{weather} versus \emph{whether}
\item \emph{you're} versus \emph{your}
\end{itemize}

All confusible correctors in Fowlt consist of two parts: \emph{TiMBL}, which is the same for every corrector, and a \emph{language model}, which is based on the specific confusible error the corrector was created for. TiMBL is machine learning software that looks at the contexts of possible confusible errors, and decides whether it would have expected that word on the basis of that context. The language model is the 'knowledge' TiMBL uses to make this decision. This knowledge was not created manually, but automatically by looking at large text corpora (so-called \emph{training material}). The more 'training material', the larger and more complete the language model, which in turn increases the likelihood that TiMBL will make the correct decision. \\\indent
But just using lots of data isn't the only thing you can do to create a good language model. Ideally, a language model is also created on the basis of a \emph{balanced} training set, i.e. a training set that consists of roughly the same amount of examples for each confusible option (see \citealt{hg09} for an overview of literature on this problem). If we would use a training set with much more contexts for option A than for option B, we may bias the model to option A. For example, if the word 'cookie' has an equal chance to occur both in the context of option A and option B, the confusible corrector should not be able to make a prediction on the basis of this word alone. However, if the language model is based on an unbalanced set it will predict option A, simply because A occurred together with 'cookie' more often. \\\indent
A solution to this could of course be to always offer balanced training sets. A balanced training set can be achieved in two ways: oversampling and undersampling \citep{provost, gh09, vhkn07, hg09}. Oversampling means that extra data are somehow created for the less frequent option. Because of the unnatural situations this creates, oversampling is probably not desirable in the spelling correction domain (and possibly for NLP in general). Undersampling means removing examples for the more frequent option: if 100 examples of option A, and 500 examples of option B are available, 400 examples of B will have to be removed. The more skewed the distribution is, the more data have to be removed, which can lead to situations where only a very small part of the originally collected training material can be used. For example, the distribution of the \emph{practice} versus \emph{practise} confusible is so unbalanced that 85.57 percent of the data had to be removed to create a balanced training set. \\\indent
This leaves us with a dilemma: on the one hand, we want to throw away data to create balanced training set, so we can improve the results, and on the other hand, we want to keep as much data as possible, so we can improve the results. This paper is an attempt to solve this dilemma. The main research question thus is:

\begin{examples}

\item Does throwing away training data to obtain a balanced data set improve the results of confusible correctors?

\end{examples}

This is investigated by creating two language models for each of the confusibles mentioned above - one on the basis of a balanced training set, and one of the basis of an unbalanced one - and evaluating their results.

% Hier even opletten of wat ikz eg wel klopt met mijn uiteindelijke resultaten

\subsection{Method}
This section is subdivided into three parts. In section \ref{timbl}, I will explain the workings of the software behind Fowlt's confusible correctors: TiMBL. This is important, because the results reported here might not hold for the confusible correctors of other spelling correctors. Section \ref{langmod} contains information about the language models  TiMBL used. Section \ref{eval}, finally, explains how the models were evaluated.


\subsubsection{TiMBL} \label{timbl}
TiMBL compares contexts. For example, when it is given the task to decide whether "your" in "I think your really nice" is a mistake or not, it compares "I think \_ really nice" to the contexts it has seen in the training material. These training contexts are divided into two groups: the contexts for the "your" option and the contexts for the "you're" contexts. The context "I think \_ really nice" is more similar to the contexts in the "you're" group, so TiMBL will predict that "you're" should be used here. Because this doesn't match with the original word, Fowlt will flag this word as an error. This approach is called the k-nearest neighbour algorithm. The specific implementation of this algorithm used for Fowlt is IGTree, a decision-tree approximation of IB1-IG \citep{dvdbw97}.

\subsubsection{The language models} \label{langmod}

The language models are the actual decision trees TiMBL uses to makes its decision. These models were created by collecting all relevant examples and their contexts from the British National Corpus (ref), and randomly removing 10\% (which were used for evaluation) of them. And example consists of the confusible word, three words to its left, and three words to its right. The algorithm used to create the models was also part of the TiMBL software package.\\\indent
As described above, language models were made for six confusibles. These confusibles were chosen because they vary in their unbalancedness. Here I summarize some descriptive statistics about them:

%ref

\begin{table}[h] \footnotesize
\begin{tabular}{|l|l|p{3cm}|l|p{3cm}|}
\hline
Confusible&Distribution&Examples in unbalanced data set&Examples in balanced set&Percentage of data lost in balanced set\\
\hline
\emph{advice} versus \emph{advise}&82.8 - 17.2&12480&4094&67.2\\
\emph{lose} versus \emph{loose}&67.7 - 32.2&9570&6178&35.44\\
\emph{practice} versus \emph{practise}&92.82 - 7.18&18407&2638&85.67\\
\emph{than} versus \emph{then}&46.97 - 53.03&302913&284546&6.06\\
\emph{weather} versus \emph{whether}&13.66 - 86.34&41747&11400&72.69\\
\emph{you're} versus \emph{your}&21.8 - 78.2&176801&77070&56.41\\
\hline
\end{tabular}
\end{table}

\subsubsection{Evaluation procedures} \label{eval}

The remaining 10\% of the collected data were used to evaluate the confusible correctors. In this section, I describe the various evaluation measures I used to do this. This is necessary because evaluation of spelling correctors has not been very uniform in the spelling checker literature so far; whereas various scholars only measured how many errors their algorithm can detect in an error list and called it accuracy (e.g. \citealp{agirre98, bm00, tm02,vandelden04}), others use or argue for the precision-recall terminology (\citealp{reynaert08,pz84}, although they use other terms for the two concepts) or argue against it \citep{sp02}.\\\indent
Although most of the evaluation measures used in the literature can be useful and informative, they only tell part of the story. I think it is important to recognize that spelling correction is a multi-step process, and each step comes with its own measures. When using a confusible corrector as part of a spelling corrector, there are three steps: word prediction, error detection and error correction. I'll go through these steps and their evaluation measures one by one in the next three paragraphs. The paragraphs after that highlight other concerns when evaluating spelling correctors.

\paragraph{Step 1: Word prediction}

\begin{itemize}
\item Prediction accuracy. Although word prediction may be the most complicated part of the confusible correction, its evaluation is actually the simplest. Calculating prediction accuracy simply entails removing all confusibles from a text, and see in how many of the cases the language model can correctly predict which word was removed. This percentage can be increased by only taking into account predictions that are above a certain threshold. 
\end{itemize}

\paragraph{Step 2: Error detection}

\begin{itemize}
\item The detection confusion matrix. From this step onwards, we concentrate on spelling errors. This means that our input no longer is errorless text, like in the previous step. We also interpret the results of the prediction differently now: whereas incorrectly predicted words were considered errors of the system in the previous task, lowering the accuracy, they will now be considered corrections. This means that the system can do two things right (correct incorrect words in the text and don't correct correct words), and two things wrong (correct correct words and don't correct incorrect words). This can be summarized in a confusion matrix (taken from \citet{fawcett04}):

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\textwidth]{confusion_matrix.png}
\caption{The confusion matrix}
\label{fig:confusion}
\end{figure}

When reporting the results of a confusible module, the names of the various cells will be replaced by numbers. These numbers are the data needed to calculate various evaluation measures, like the three below. Beside the evaluation measures, I will always give the confusion matrix itself, so readers can calculate other measures if desired.

\item Detection accuracy. Detection accuracy only indicates how many mistakes the confusible detector makes, not what kind of mistakes. It can be calculated from the confusion matrix with the following formula:

\[
\frac{tp+tn}{tp+tn+fp+fn}
\]

It is similar to prediction accuracy in that it indicates how well the sytem does in general, but keep in mind that prediction and detection accuracy measure two completely different concepts. Although a system with a good prediction accuracy is likely to also have a good detection accuracy, it might be possible that a system very good at predicting confusibles in errorless texts behaves differently in texts that do contain errors.
 
\item Detection recall and detection precision. 
Precision and recall are concepts well-known within the field of Information Retrieval; precision reflects how much of the retrieved things were relevant, while recall reflects how much of the the relevant things were retrieved. Often there is a inverse relationship between precision and recall: if you change an algorithm so that its recall increases, its precision decreases, and the other way around. In case of spelling checkers, this means you either have a system that catches a lot of errors, but also has a lot of false alarms, or a system that only finds true errors, but also misses a lot; you can't have the best of both. \\

Precision

\[
\frac{tp}{tp+fn}
\]

Recall

\[
\frac{tp}{tp+fp}
\]

\item Detection F-score. 
Although you mostly cannot have an optimal precision \emph{and} recall at the same time, you can of course try to have them both as high as possible. Calculating the mean does not help here, because the mean can be relatively high if one of the two is high, but what you want to know is whether \emph{both} are high. This is fixed by the F-score, calculated with the following formula.

\[
2 * \frac{precision * recall}{precision + recall}
\]

Because this F-score 'summarizes' both precision and recall, it comes close to a measure that indicates how well the system does with it is supposed to do, and that is of course what we are interested in for this research question. For this reason, I will leave precision and recall out, and focus on the F-scores.

\end{itemize}

\paragraph{Step 3: Error correction.} Detecting an error is of course something different from correctly correcting it. That is, when a spelling checker corrects 'i like you alot' into 'i like you allot' it might have detected the error, but replaced it with the wrong word. If approriate corrections is what the user is interested in, it is possible to only count as true positives the errors correctly corrected, while the incorrectly corrected errors are counted as false positives. From the resulting correction confusion matrix the measures correction accuracy, correction recall, correction precision and correction F-score can then be calculated again. \\\indent
For this paper, the correction measures will always be identical to the detecion measures, and will therefore not be mentioned seperately. This is because only two-option confusibles are investigated, which means that the approriate correction is always 'the other option' once an error is detected correctly. This of course does not hold for confusibles with three or more options, like \emph{to} versus \emph{too} versus \emph{two} or \emph{they're} versus \emph{there} versus \emph{their}.

\paragraph{Simulating errors.} In the description of these measures, I have assumed the existence of a text corpus full of annotated natural errors, so we can easily see which errors were correctly identified, and which were missed. Such a corpus unfortunately does not exist, and because of time and money constraints I was unable to create one myself. To solve this, such a corpus was created artificially by adding errors to 10\% of the examples used for evaluation. I would like to stress, however, that this is a dangerous approach: I risk adding very unnatural errors, making the evaluation results different from real usage results, and thus less useful. \\\indent
Fortunately, our focus on confusibles largely eliminates this problem: we don't only already know which words are often spelled wrong, we also know \emph{how} language users spell them wrong (unlike, for example, typos). A problem that still remains, though, is the fact that some errors are not made as often as others. For example, "your" might be used instead of "you're" much more than vice versa; \citet{sfd99} for example claim that confusible errors are largely based on frequencies. Research into the occurence frequencies for all confusible errors would be needed to be able to simulate errors in a more natural way.

%ref

\paragraph{Thresholds.} As mentioned in the section about prediction accuracy, predictions come with a confidence, and it is possible to only take into account predictions above a particular threshold. This decision has an enormous influence on the rest of the confusible correction process, as in the detection phase every prediction below the threshold means a change either from a true positive into a false negative or from a false positive into a true negative. Because a higher threshold means the corrector has to be more confident to flag a word as an error, raising the threshold essentially means lowering the amount of false positives at the cost of lowering the amount of true positives; in other words, increasing the precision at the cost of recall. To find out at which threshold there is an optimal precision-recall ratio, the F-score is calculated for multiple thresholds, as can be seen in the results section.

\paragraph{Problems with precision and recall.} Although I limit myself to precision, recall, and the F-scores calculated from them, these measures are not without problems. As can be seen from the formulas given above, for example, they completely ignore the 'true negative' box in the confusion matrix. As \citet{fawcett04} shows, this can have quite extreme implications when the class distribution changes, and proposes ROC curves instead; this entails by pitting the true positive rate against the false positive rate. However, for spelling correction this measure is not useful: because the amount of negatives (correct words) usually is many times larger than the amount of positives (incorrect words), this always leads to false positive rates that are near-zero. This in turns causes the Area Under the Curve is almost completely determined by the recall, which is of course only part of what we're interested in.

\subsection{Results}

In this section I present the F-scores of all confusible correctors, calculated for 21 different thresholds. Because all examples were assigned to either the training set and the test set randomly (as was explained in the Methods section, 90\% of the data were used to train the correctors, while 10\% was used to calculated these measures), and also because the errors added to the test set were added randomly, these results are not exactly reproducable: a confusible corrector trained on other examples and tested on other errors might of course have other results. Nevertheless, a quick test showed that the F-scores are relatively stable: for all correctors, when the procedure was done again, the F-scores were less than 0.01 higher or lower, which suggest that the results presented here are good reflections of the correctors' actual performance.\\\indent
Prediction accuracy for multiple thresholds and confusion matrices were calculated as well. These can be found in the appendix.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{fscore_adviceadvise.png}
\caption{The F-scores of corrector for the \emph{advice} versus \emph{advise}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{fscore_loseloose.png}
\caption{The F-scores of corrector for the \emph{lose} versus \emph{loose}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{fscore_practicepractise.png}
\caption{The F-scores of corrector for the \emph{practice} versus \emph{practise}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{fscore_thanthen.png}
\caption{The F-scores of corrector for the \emph{than} versus \emph{then}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{fscore_weatherwhether.png}
\caption{The F-scores of corrector for the \emph{weather} versus \emph{whether}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{fscore_you'reyour.png}
\caption{The F-scores of corrector for the \emph{you're} versus \emph{your}, trained on both a balanced and an inbalanced data set.}
\end{figure}

With respect to our research question, two important facts can be distilled from these data:

\begin{itemize}

\item The corrector trained on the balanced data set (the blue line in the graph) always performs similar or worse than the unbalanced set. Only for the \emph{lose} versus \emph{loose} corrector we see that the balanced set provides the best results. However, the difference between the F-scores of the balanced and unbalanced set in less than 0.01, which means that this better performance can attributed to chance.
\item How much better the corrector trained on the unbalanced data set actually is seems to be related to how much data was lost. For example, for the balanced set for the \emph{than} versus \emph{then} corrector only 6.06\% of the data had to be thrown away, and we indeed see that the correctors based on the balanced and unbalanced perform similar. For the \emph{practice} versus \emph{practise} corrector, on the other hand, 85.67\% of the data had to be thrown away, and here was see that the corrector trained on the large, unbalanced data set performs much better. Spearman's rho\footnote{Spearman's rho was chosen over Pearson's r, because an early plot of the data indicated that the correlation was not linear.} indicates that there is indeed a strong positive correlation between difference in F-score and proportion of data lost ($r_s(4)=.812, p=.05$). This means that the more data that are lost, the more the corrector based on the unbalanced set will outperform the corrector based on the balanced data set.

\end{itemize}

\subsection{Conclusion}
In this study, six confusible correctors were trained on both a balanced and an inbalanced data set. It was found that the correctors using the imbalanced training data always performed similar and mostly better than the balanced training data. A strong correlation was found between how much data had to be removed to create the balanced data set and to what extent the corrector based on the imbalanced training data outperformed its balanced counterpart. This suggests that the dataset being large is more important than the dataset being balanced. These findings form a contrast with suggestions in the literature that imbalanced training data should be avoided.

\section{General conclusion}
In this research training portfolio, I hope to have shown that creating a spelling corrector isn't something that can be done in a day, even if most of the code is already available. I didn't only change what was there to work with English, I also had to solve several problems related to processing the English language, extend the software on some places (for example, with the confidence slider), and did a little study on the confusible correctors' training data. While doing this, I had to get familiar with new concepts (SSH, GitHub, MBL) on an almost daily basis.\\\indent
Perhaps suprisingly, the fact that I have created something which could (hopefully) be useful to hundreds, if not thousands or millions of people around the globe, and the fact that I have gathered lots of new and super useful knowledge, are not the most valuable things I got out of this internship. The most valuable thing is that I rediscovered language technology as something I love, and that I now know what I want to spend my future doing.

\section{Appendix}

\subsection{Advice versus advice corrector}

\begin{table}[H] \footnotesize
\centering
\begin{tabular}{|c|c|c|c|}
\hline
&&P&N\\
\hline
balanced&Y&0&0\\
&N&0&0\\
\hline
\hline
unbalanced&Y&0&0\\
&N&0&0\\
\hline
\end{tabular}
\caption{The confusion matrices for the corrector for the \emph{advice} versus \emph{advise}, trained on both a balanced and an inbalanced data set.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{accuracy_adviceadvise.png}
\caption{The accuracies of corrector for the \emph{advice} versus \emph{advise}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\subsection{Lose versus loose corrector}

\begin{table}[H] \footnotesize
\centering
\begin{tabular}{|c|c|c|c|}
\hline
&&P&N\\
\hline
balanced&Y&0&0\\
&N&0&0\\
\hline
\hline
unbalanced&Y&0&0\\
&N&0&0\\
\hline
\end{tabular}
\caption{The confusion matrices for the corrector for the \emph{lose} versus \emph{loose}, trained on both a balanced and an inbalanced data set.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{accuracy_loseloose.png}
\caption{The accuracies of corrector for the \emph{lose} versus \emph{loose}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\subsection{Practice versus practise corrector}

\begin{table}[H] \footnotesize
\centering
\begin{tabular}{|c|c|c|c|}
\hline
&&P&N\\
\hline
balanced&Y&0&0\\
&N&0&0\\
\hline
\hline
unbalanced&Y&0&0\\
&N&0&0\\
\hline
\end{tabular}
\caption{The confusion matrices for the corrector for the \emph{practice} versus \emph{practise}, trained on both a balanced and an inbalanced data set.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{accuracy_practicepractise.png}
\caption{The accuracies of corrector for the \emph{practice} versus \emph{practise}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\subsection{Than versus then corrector}

\begin{table}[H] \footnotesize
\centering
\begin{tabular}{|c|c|c|c|}
\hline
&&P&N\\
\hline
balanced&Y&0&0\\
&N&0&0\\
\hline
\hline
unbalanced&Y&0&0\\
&N&0&0\\
\hline
\end{tabular}
\caption{The confusion matrices for the corrector for the \emph{than} versus \emph{then}, trained on both a balanced and an inbalanced data set.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{accuracy_thanthen.png}
\caption{The accuracies of corrector for the \emph{than} versus \emph{then}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\subsection{Weather versus whether corrector}

\begin{table}[H] \footnotesize
\centering
\begin{tabular}{|c|c|c|c|}
\hline
&&P&N\\
\hline
balanced&Y&0&0\\
&N&0&0\\
\hline
\hline
unbalanced&Y&0&0\\
&N&0&0\\
\hline
\end{tabular}
\caption{The confusion matrices for the corrector for the \emph{weather} versus \emph{whether}, trained on both a balanced and an inbalanced data set.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{accuracy_weatherwhether.png}
\caption{The accuracies of corrector for the \emph{weather} versus \emph{whether}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\subsection{You're versus your corrector}

\begin{table}[H] \footnotesize
\centering
\begin{tabular}{|c|c|c|c|}
\hline
&&P&N\\
\hline
balanced&Y&0&0\\
&N&0&0\\
\hline
\hline
unbalanced&Y&0&0\\
&N&0&0\\
\hline
\end{tabular}
\caption{The confusion matrices for the corrector for the \emph{you're} versus \emph{your}, trained on both a balanced and an inbalanced data set.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{accuracy_you'reyour.png}
\caption{The accuracies of corrector for the \emph{you're} versus \emph{your}, trained on both a balanced and an inbalanced data set.}
\end{figure}

\begin{thebibliography}{99}

\bibitem[Agirre et al., 1998]{agirre98}
Agirre, E., K. Gojenola, K. Sarasola, \& A. Voutilainen (1998). Towards a Single Proposal in Spelling Correction. \emph{COLING-ACL ’98},  22-28.
\bibitem[Brill \& Moore, 2000]{bm00}
Brill, E. \& R. C. Moore (2000). An improved error model for noisy channel spelling correction. In \emph{Proceedings of the 38th Annual Meeting of the ACL}, 286-293.
\bibitem[Daelemans, Van den Bosch \& Weijters, 1997]{dvdbw97}
Daelemans, W., A. van den Bosch, \& A. Weijters (1997). IGTree: using trees for compression and classification in lazy learning algorithms. \emph{Artificial Intelligence Review, 11}, 407–423.
\bibitem[Van Delden et al., 2004]{vandelden04}
Van Delden, S., D. B. Bracewell, \& F. Gomez (2004). Supervised and unsupervised automatic spelling correction algorithms. In D. Zhang, E. Gregoire \& D. DeGroot (eds), \emph{Proceedings of the 2004 IEEE International Conference on Information Reuse and Integration}, 530–535, Las Vegas.
\bibitem[Fawcett, 2004]{fawcett04}
Fawcett, T. (2004). \emph{ROC Graphs: Notes and Practical Considerations for Researchers.} Available online at binf.gmu.edu/mmasso/ROC101.pdf.
\bibitem[Garcia \& Herrera, 2009]{gh09}
Garcia, S. \& F. Herrera (2009). Evolutionary Undersampling for Classification with Imbalanced Datasets: Proposals and Taxonomy. \emph{Evolutionary Computation 17}, 275-306.
\bibitem[He \& Garcia, 2009]{hg09}
He, H. \& E. Garcia (2009). Learning from imbalanced data. \emph{IEEE Transactions on knowledge and data engineering, 21(9)}, 1263-1281.
\bibitem[Van Hulse, Khoshgoftaar \& Napolitano, 2007]{vhkn07}
Van Hulse, J. T. M. Khoshgoftaar \& A. Napolitano (2007). Experimental Perspectives on Learning from Imbalanced Data. \emph{Proceedings of the 24 th International Conference on Machine Learning.}
\bibitem[Pollock \& Zamora, 1984]{pz84}
Pollock J. J. \& A. Zamora (1984). Automatic spelling correction in scientific and scholarly text. \emph{Commun. ACM, 27(4)}, 358-368.
\bibitem[Provost, ?]{provost}
Provost, F. (year unkown). \emph{Machine Learning from Imbalanced Data Sets 101.} Extended abstract.
\bibitem[Reynaert, 2008]{reynaert08}
Reyneart, M. (2008). All, and only, the errors: More complete and consistent spelling and OCR-error correction evaluation. \emph{Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco}.
\bibitem[Sandra et al., 1999]{sfd99}
Sandra, D., Frisson, S.,Daems, F., (1999). Why simple verb forms can be so difficult to spell: the influence of homophone frequency and distance in Dutch. \emph{Brain Language, 68(1-2),} 277-283.
\bibitem[Starlander \& Popescu-Belis, 2002]{sp02}
Starlander, M. \& A. Popescu-Belis (2002). Corpusbased evaluation of a French spelling and grammar checker. In \emph{LREC 2002: Third International Conference on language resources and evaluation, volume 1}, 268–274. Paris: ELRA, European Language Resources.
\bibitem[Toutanova \& Moore, 2002]{tm02}
Toutanova, K. \& R. C. Moore (2002). Pronunciation modeling for improved spelling correction. In \emph{Proceedings of the 40th Annual Meeting of the ACL}, 144–151.

\end{thebibliography}

% Ene paper Antal

\end{document}
