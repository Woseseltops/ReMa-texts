\documentclass[12pt]{article}

\title{Twitter as a crystal ball: detecting and predicting real-world events with Twitter}
\author{Wessel Stoop, s0808709}

\usepackage{covington}
\usepackage{graphicx}
\usepackage{apacite}
\usepackage{float}

\renewcommand{\familydefault}{\sfdefault}

\begin{document}
\maketitle

\section{Introduction}
Over the last two decades, the way people exchange information has changed considerably. Social media have become a common way communicate for many people: at the time of writing, social networking service Facebook\footnote{\url{http://www.facebook.com}} claims to have 665 million daily active users, whereas micro-blogging service Twitter\footnote{\url{http://www.twitter.com}} claims to have 340 million Tweets per day. Interestingly, this communciation is not limited to peer-to-peer communication; communication via social media makes it possible for an individual to reach quite large audiences easily. 

This possibility to reach a lot of people quickly is important change in news reporting. Whereas people used to read about most events happening in the world in the newspaper, or hear about it on the television news, they can now hear it from their friends. On top of that, they can report events they encounter themselves. This essentially transforms social media users from news consumers to a giant group of news reporters, reporting their experiences to eachother. As \citeA{rui+12} put it, '[content] is produced by a large number of ordinary Internet users, rather than by only a few publishers and television networks, as in the traditional media. This ongoing shift to social media means that user-generated content is now playing an unprecedented role in people’s life and likely will completely alter the way information is generated in our society in the future.'

However, unlike professional news reporters, most people have no financial benefits of reporting the news well. How accurate, clear and useful is social media communication concerning recent, current or upcoming events? \citeA{rui+12} investigated the reasons why people provision public goods (like Tweets), and propose a theory where social media (like Twitter) are depicted as 'attention exchange platforms': since in our digital society, every hour more \emph{information} is produced than an individual can process in a lifetime, human \emph{attention} has become a scarce resource. Social media can help people to collect information that interests them (i. e. people with attention looking for information), whereas others use it spread information (i. e. people with information looking for attention). The most important reason for the desire to spread information seems to be social status; \citeA{rui+12} show that Twitter users with long biograpies - people that have written a lot about themselves for others to read, and thus are likely to be interested in improving their status - tweet more often, and that for most users the tweeting rate increases as the number of followers increases.

%abs. getallen?

If social status is involved, at least part of the social media communication is very likely be meaningful to some people. But are they meaningful enough to when it comes to detecting events that are going to happen in the future? And more importantly, can this prediction be done automatically? Research on other forms of communication suggest it can. For example, \citeA{veronis07} shows that for the French presidential elections of 2007 simply counting the numbers of candidate mentions in the press was a better prediction of success than many polls. Similarly, \shortciteA{gruhl+05} were able to predict when there would be spikes in the number of book sales on amazon.com\footnote{\url{http://www.amazon.com}} with an accuracy of [[x]]. By using the frequency of mentions of a particular book title on IBM's webfountain, which collects its content from a large number of weblogs, their algorithm could predict a rapid increase in the number of sales 1-2 days before it would happen.

%abs

The aim of this paper is to give an overview of studies that predict real-world events on the basis of Twitter data. Section \ref{success} will summarize various studies which succeeded in this task to a certain extent. Section \ref{problems} will address some critique and problems for the field. The paper will be concluded in section \ref{conclusion}.

\section{Success stories} \label{success}

Twitter is micro-blogging service. Each Twitter user submits status updates known as tweets, which consist 140 characters or less. These tweets typically consist of personal information about the users themselves, news or links to other pages on the internet. Tweets are displayed on the user’s profile page, as well as shown to his/her followers. In this section, four studies will be discussed which have succesfully detected or predicted events on the basis of the language in tweets alone. Although the terminology, 'detecting and predicting events', might suggest this paper is about two separate tasks, they are actually related in several ways. For example, if we detect a current event, this will almost always lead to assumptions about the future, and doing predictions about future event $x$ can only be done by detecting the current event that people are talking about $x$. For this reason, while discussing the literature, detecting and predicting events will be treated as a continuum. We will start on one end of this continuum (detecting ongoing events that have started in the past) and step by step move to the other (predicting events far into the future).

\citeA{lampos+10} searched the Twitter API for tweets for which gps coordinates were provided, and which contained one or more of various keywords related to the flu (\emph{cough}, \emph{fever}, \emph{tired}, etc.). Using the coordinates, these tweets were linked to a particular area. For five areas in the United Kingdom, they then counted how much tweets contained one or more of these words, and compared these numbers to official data about the spread of the flu from the Health Protection Agency (HPA). Although there were some differences per area, on average a statistically significant linear correlation coefficient greater than 89\% could be achieved. For example, a lot more tweets containing health-related words were coming from areas were HPA discovered an increase in the number of people affected with the flu. Furthermore, by using correlation data, they automatically improved their set of queries. This resulted in an average correlation coefficent of 95\%.  Interestingly, it takes HPA two weeks to gather and analyze the necessary data, while the results of \citeA{lampos+10} are available directly. That is, their algorithm can quite accurately detect where the flu prevails \emph{right now}.

Similarly focussing on ongoing events, \shortciteA{sakaki+10} built an algorithm that detects earthquakes on the fly. According to \shortciteA{sakaki+10}, 'the government has allocated a considerable amount of its budget to mitigating earthquake damage', since earthquakes occur so regularly there. The system they have built uses the Twitter search API to search for queries like 'earthquake' and 'shaking', and then uses a classifier to find out whether this tweet really is about earthquakes (and not about, for instance, an earthquake conference). As features for this classifier, they use (1) statistical characteristics (the number of words and the position of the query words), (2) the other words in the tweet and (3) the words before and after the keyword. A model using all of these features turned out the most succesful. When a positive tweet has been detected with a confidence value higher than .99, the system reports an event. This way, they were able to detect 96\% of the earthquakes stronger than JMA seismic intensity scale 3 or more. On top of that, they try to estimate \emph{where} the event is happening, by using the gps coordinates given with the tweets (if any) and the twitter user's home location. They put all positive tweets in a particle filter, and are this way able to have a very close approximation of the earthquake center.

 Importantly, earthquakes do not reach all places at the same time; instead, they start at one location and then gradually spread to other places. This means that if an event is detected for one location, it can function as a prediction as another. The algorithm by \citeA{sakaki+10} sends warnings to people who have subscribed to the service. According to them 'an earthquake is transmitted through the earth’s crust at about 3–7 km/s. Therefore, a person has about 20 seconds before its arrival at a point that is 100 km distant. [...] The delivery time is far faster than the rapid broadcast of announcements of JMA, which are widely broadcast on TV; on average, a JMA announcement is broadcast 6 min after an earthquake occurs.'

\citeA{asur+10} concentrate on doing predictions a week into the future: their algorithm uses tweets to predict the box-office revenue of movies released next week. The gold standard for such predictions so far is the Hollywood Stock Exchange Index (HSX). The HSX\footnote{\url{http://www.hsx.com}} is a web-based multiplayer game in which players use simulated money to buy and sell "shares" of actors, directors and upcoming films. Since these shares can be exchanged for this simulated money [[two weeks]] after the movies have been released, it is the job of the players to buy them for less money than they produced (or to sell them for more than they costed or will produce). How much money these shares produce depends on the box-office revenue of the corresponding movies, which results in hundreds of people trying to guess it as accurately as possible. Combined with the number of theathers the movie is released in, a factor which of course has a very big influence on how much revenue can possibly be made, the HSX correlates with the actual revenue with an $R^2$ of .965.

By using tweet rates, which is the number of tweets per hour, \citeA{asur+10} were able to improve this result. Their algorithm looks for tweets that contain the title of a movie. The number of tweets found is then divided by the number of hours, which results in the tweet rate. This rate will be high for movies which have either large burst of attention, or when there a lot of bursts, but low otherwise. Attention bursts are typically caused by the release of new promotional material \cite{asur+10}. When only using tweets, \citeA{asur+10} found a correlation of $R^2 = .92$ between tweet rate and the actual revenue. Adding the number of theaters resulted, as was done with HSX, resulted in the even higher $R^2$ of .973. The correlation between actual revenue and tweet rate was higher than the correlation between actual revenue and HSX for every movie in the study. \citeA{asur+10} also investigated to what extent adding sentiment analysis can improve the results. They used the DynamicLMClassifier with sentiment analyses produced by \emph{Amazon Mechanical Turk}. This improved results, but only for the second weekend (from .92 to .94 $R^2$), which according to the authors is caused by reviews and word of mouth. For example, the movie \emph{The Blind Side}, did not perform well in the opening weekend (34M), but got a positive response: it showed an enormous increase in positive sentiment from 5.02 to 9.65. It indeed performed better in its second weekend (40.1M). 

\shortciteA{Tumasjan+10} finally do predictions for an event several weeks beforehand: the elections. They have collected all tweets mentioning one of the six larger parties (CDU/CSU, SPD, FDP, B90/Die Grünen, and Die Linke) posted in the month before the German national elections of 2009. They also collected tweets mentioning several politicians linked to one of these parties. The percentage of tweets in this sample for each party was almost identical to the percentage of votes. \shortciteA{Tumasjan+10} report a \emph{Mean Absolute Error}, a measure to calculate to what extent the percentages are similar, of 1.65\%. Although traditional polls get even lower Mean Absolute Errors (ranging from for 0.80\% to 1.45\%), this is more than enough to establish the relative order of the parties in terms of number of votes. 

\section{Problems} \label{problems}

With regard to elections, some authors have taken the opposite stance [[ref]]. Most fierce in this respect is \citeA{gayo12}, who claims 'No, you cannot predict elections with Twitter'. He goes on to point out a number 'flaws' in research on doing predictions with Twitter. A summary of his most important points:

\begin{enumerate}
\item Scholars should actually predict something, instead of doing \emph{post-hoc} analyses. As \citeA{gayo12} put it: 'if you are claiming you have a prediction method you should predict an election in the future!'
\item 'Not all tweets are trustworthy.' That is, some might be jokes or sarcasm and some might have been placed by the political leaders themselves. These tweets should not be taken into account.
\item 'Demographics are neglected.' Twitter users are likely to be younger, higher-educated people interested in social media, which might not be a representative sample of the population . Researchers should correct for this bias somehow.
\item 'There is not a commonly accepted way of “counting votes” in Twitter'. Whereas some authors use tweet volumes, others use tweet rate, and again other focus on the results of some kind of sentiment analyses.
\end{enumerate}

However, I am convinced all of these points are not really flaws in the research, but more general problems related to predicting the future with Twitter. As for the first point, you cannot evaluate a prediction system if you do not use past events; a paper claiming 'system X predicts event Y', without the information where Y actually happened, is meaningless. As long as the authors only used material available \emph{before} event Y, their results are perfectly valid. Arguments 2 and 3 are about noise in the data, which I think is not only a general problem in Twitter research, but a problem in science in general; datasets which are 100\% representative of thing they were created to be representative of are scarce. The task of predicting events with Twitter is to do accurate predictions \emph{despite} these problems. If researchers can even build useful systems and report positive results by ignoring them, why bother? Finally, the last 'flaw' is actually the main goal of this research field: finding which way of 'counting votes' has the best correlation to the actual election results. Because it is not clear yet what technique consistently has the best results, various ideas will be proposed. The fact that everybody uses another way to count votes thus is not a problem, but an attempt for a solution.

I therefore feel it would be more appropriate to rephrase the first three points of critique of \citeA{gayo12} as problems common to this research field:

\begin{enumerate}
\item To be able to tell whether a prediction system works, one is mostly limited to events of the past. If a prediction is done on the basis of very recent tweets, it might take some time before one can be sure this prediction was accurate.
\item Some tweets might use figurative language or be misleading in another way, and this way put automated systems on the wrong track.
\item Twitter users are probably not representative of the population. Although it is hard to retrieve exact numbers, \shortciteA{mislove+11} are able to do some estimates on the basis of the data available. Among other things, they (1) discovered that more populous areas are overrepresented on Twitter (so the percentage of the population that uses Twitter is much higher in these areas) and (2) estimated, on the basis of self-reported first names, that 72\% of the Twitter users are male, although this percentage seems to decrease over the years.
\end{enumerate}

% GROVE DATA
% VEEL MENSEN PRATEN NIET

\section{Conclusion} \label{conclusion}

\bibliography{twitbib}{}
\bibliographystyle{apacite}

\end{document}

%TODO
% Niet alleen predicting